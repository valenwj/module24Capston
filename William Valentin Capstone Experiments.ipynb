{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ff65c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\viewh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\viewh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\viewh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, recall_score, classification_report, precision_score, f1_score\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72018a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "0       https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1       https://www.huffpost.com/entry/american-airlin...   \n",
       "2       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3       https://www.huffpost.com/entry/funniest-parent...   \n",
       "4       https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "...                                                   ...   \n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline   category  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                     authors        date  \n",
       "0       Carla K. Johnson, AP  2022-09-23  \n",
       "1             Mary Papenfuss  2022-09-23  \n",
       "2              Elyse Wanshel  2022-09-23  \n",
       "3           Caroline Bologna  2022-09-23  \n",
       "4             Nina Golgowski  2022-09-22  \n",
       "...                      ...         ...  \n",
       "209522      Reuters, Reuters  2012-01-28  \n",
       "209523                        2012-01-28  \n",
       "209524                        2012-01-28  \n",
       "209525                        2012-01-28  \n",
       "209526                        2012-01-28  \n",
       "\n",
       "[209527 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accesing News_Category_Dataset\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\viewh\\\\OneDrive\\\\Documents\\\\News_Category_Dataset_v3.json'\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at the path: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823dda8a",
   "metadata": {},
   "source": [
    "Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a287521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty cells\n",
    "df = df[df['char_count'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea206c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of categories\n",
    "category_count = df['category'].nunique()\n",
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5e39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce categories to 16 for computational simplification\n",
    "# Combine some features and remove others\n",
    "df['category'] = df['category'].replace('COLLEGE', 'EDUCATION')\n",
    "df['category'] = df['category'].replace('ARTS', 'ARTS & CULTURE')\n",
    "df['category'] = df['category'].replace('CULTURE & ARTS', 'ARTS & CULTURE')\n",
    "df['category'] = df['category'].replace('WELLNESS', 'HEALTHY LIVING')\n",
    "df['category'] = df['category'].replace('PARENTS', 'PARENTING')\n",
    "df['category'] = df['category'].replace('STYLE', 'STYLE & BEAUTY')\n",
    "df['category'] = df['category'].replace('GREEN', 'ENVIRONMENT')\n",
    "values_to_remove = ['WEIRD NEWS', 'FOOD & DRINK', 'U.S. NEWS', 'WORLD NEWS',  'MONEY', 'QUEER VOICES', 'BLACK VOICES', 'TECH', 'MEDIA', 'LATINO VOICES', 'COMEDY', 'IMPACT', 'TASTE', 'GOOD NEWS', 'THE WORLDPOST', 'WORLDPOST', 'WEDDINGS', 'DIVORCE', 'FIFTY']\n",
    "df = df[~df['category'].isin(values_to_remove)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e1a2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count = df['category'].nunique()\n",
    "category_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a8bfc",
   "metadata": {},
   "source": [
    "Text Preprocessing\n",
    "Tokenize headline, remove stop words, punctuation and capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53336235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'headline' as the input feature and 'category' as the target\n",
    "X = df['headline']\n",
    "y = df['category']\n",
    "\n",
    "# Split with the data for training and testing desired test_size\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# Split the data while maintaining class distribution\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    train_indices.append(train_index)\n",
    "    test_indices.append(test_index)\n",
    "\n",
    "# Use the indices to create the training and testing sets\n",
    "X_train = X[train_indices[0]]\n",
    "X_test = X[test_indices[0]]\n",
    "y_train = y[train_indices[0]]\n",
    "y_test = y[test_indices[0]]\n",
    "\n",
    "# Tokenization and Text Normalization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply Tokenization to each row in X_train and store in X_train_preprocessed\n",
    "num_samples = X_train.shape[0]\n",
    "X_train_preprocessed = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    row = X_train.iloc[i]\n",
    "    preprocessed_text = preprocess_text(row)\n",
    "    X_train_preprocessed.append(preprocessed_text)\n",
    "\n",
    "# Apply Tokenization to each row in X_test and store in X_test_preprocessed\n",
    "num_samples = X_test.shape[0]\n",
    "X_test_preprocessed = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    row = X_test.iloc[i]\n",
    "    preprocessed_text = preprocess_text(row)\n",
    "    X_test_preprocessed.append(preprocessed_text)\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Vectorize the preprocessed text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english') \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f1531d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best alpha: 0.1\n",
      "             Metric  NB Results\n",
      "0        Best Alpha    0.100000\n",
      "1   Train Precision    0.826818\n",
      "2      Train Recall    0.812990\n",
      "3    Train F1-Score    0.805137\n",
      "4    Train Accuracy    0.812990\n",
      "5    Test Precision    0.697963\n",
      "6       Test Recall    0.690125\n",
      "7     Test F1-Score    0.667307\n",
      "8     Test Accuracy    0.690125\n",
      "9  Grid Search Time    2.162459\n",
      "Classification Report by Category (Train Data) for the Best Hyperparameter:\n",
      "                Train Precision  Train Recall  Train F1-Score  Train Support\n",
      "ARTS & CULTURE         0.951368      0.598661        0.734886     3137.00000\n",
      "BUSINESS               0.893784      0.546005        0.677891     4793.00000\n",
      "CRIME                  0.891880      0.732281        0.804239     2850.00000\n",
      "EDUCATION              0.935821      0.363268        0.523372     1726.00000\n",
      "ENTERTAINMENT          0.816788      0.866657        0.840984    13889.00000\n",
      "ENVIRONMENT            0.880489      0.597910        0.712193     3253.00000\n",
      "HEALTHY LIVING         0.739482      0.890822        0.808128    19711.00000\n",
      "HOME & LIVING          0.934790      0.767361        0.842841     3456.00000\n",
      "PARENTING              0.803583      0.743454        0.772350    10197.00000\n",
      "POLITICS               0.775705      0.950423        0.854222    28481.00000\n",
      "RELIGION               0.961646      0.498787        0.656869     2061.00000\n",
      "SCIENCE                0.960922      0.543343        0.694173     1765.00000\n",
      "SPORTS                 0.920308      0.793401        0.852156     4061.00000\n",
      "STYLE & BEAUTY         0.878921      0.867723        0.873286     9654.00000\n",
      "TRAVEL                 0.863591      0.864899        0.864244     7920.00000\n",
      "WOMEN                  0.908012      0.321204        0.474541     2858.00000\n",
      "accuracy               0.812990      0.812990        0.812990        0.81299\n",
      "macro avg              0.882318      0.684137        0.749148   119812.00000\n",
      "weighted avg           0.826818      0.812990        0.805137   119812.00000\n",
      "\n",
      "Classification Report by Category (Test Data) for the Best Hyperparameter:\n",
      "                Test Precision  Test Recall  Test F1-Score  Test Support\n",
      "ARTS & CULTURE        0.744409     0.296815       0.424408    785.000000\n",
      "BUSINESS              0.673250     0.313022       0.427350   1198.000000\n",
      "CRIME                 0.742616     0.494382       0.593592    712.000000\n",
      "EDUCATION             0.681818     0.104167       0.180723    432.000000\n",
      "ENTERTAINMENT         0.681299     0.773107       0.724305   3473.000000\n",
      "ENVIRONMENT           0.676845     0.327183       0.441128    813.000000\n",
      "HEALTHY LIVING        0.639605     0.800933       0.711235   4928.000000\n",
      "HOME & LIVING         0.815486     0.572917       0.673012    864.000000\n",
      "PARENTING             0.658600     0.597882       0.626774   2549.000000\n",
      "POLITICS              0.681066     0.918961       0.782328   7120.000000\n",
      "RELIGION              0.791946     0.229126       0.355422    515.000000\n",
      "SCIENCE               0.831325     0.312925       0.454695    441.000000\n",
      "SPORTS                0.804225     0.562008       0.661645   1016.000000\n",
      "STYLE & BEAUTY        0.769263     0.760978       0.765098   2414.000000\n",
      "TRAVEL                0.740061     0.733333       0.736682   1980.000000\n",
      "WOMEN                 0.669118     0.127451       0.214118    714.000000\n",
      "accuracy              0.690125     0.690125       0.690125      0.690125\n",
      "macro avg             0.725058     0.495324       0.548282  29954.000000\n",
      "weighted avg          0.697963     0.690125       0.667307  29954.000000\n"
     ]
    }
   ],
   "source": [
    "#MULTINOMIAL \n",
    "\n",
    "# Define a grid of hyperparameters for MultinomialNB\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0, 10.0, 100.0], \n",
    "}\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# GridSearchCV cross-validation and scoring \n",
    "grid_search_nb = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', refit=True)\n",
    "\n",
    "nbstart = time.time()\n",
    "grid_search_nb.fit(X_train_tfidf, y_train_encoded)\n",
    "nbstop = time.time()\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_clf = grid_search_nb.best_estimator_\n",
    "nb_best_alpha = best_clf.alpha\n",
    "\n",
    "# Predict categories for the training set\n",
    "y_train_pred = best_clf.predict(X_train_tfidf)\n",
    "\n",
    "# Predict categories for the test set\n",
    "y_test_pred = best_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Train set results\n",
    "nb_precision_train = precision_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "nb_recall_train = recall_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "nb_f1_train = f1_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "nb_train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
    "\n",
    "# Test set results\n",
    "nb_precision_test = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "nb_recall_test = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "nb_f1_test = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "nb_test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "nb_time = nbstop - nbstart\n",
    "\n",
    "# Results\n",
    "results_data = {\n",
    "    'Metric': ['Best Alpha', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Accuracy',\n",
    "               'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Accuracy', 'Grid Search Time'],\n",
    "    'NB Results': [nb_best_alpha, nb_precision_train, nb_recall_train, nb_f1_train, nb_train_accuracy,\n",
    "              nb_precision_test, nb_recall_test, nb_f1_test, nb_test_accuracy, nb_time]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "\n",
    "# Calculate classification report metrics for train and test data\n",
    "train_classification_report = classification_report(y_train_encoded, y_train_pred, target_names=class_names, output_dict=True)\n",
    "test_classification_report = classification_report(y_test_encoded, y_test_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "train_classification_df = pd.DataFrame(train_classification_report).transpose()\n",
    "test_classification_df = pd.DataFrame(test_classification_report).transpose()\n",
    "\n",
    "train_classification_df.rename(columns={'precision': 'Train Precision', 'recall': 'Train Recall', 'f1-score': 'Train F1-Score', 'support': 'Train Support'}, inplace=True)\n",
    "test_classification_df.rename(columns={'precision': 'Test Precision', 'recall': 'Test Recall', 'f1-score': 'Test F1-Score', 'support': 'Test Support'}, inplace=True)\n",
    "\n",
    "# Print Results\n",
    "\n",
    "print(\"\\nBest alpha:\", nb_best_alpha)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(\"Classification Report by Category (Train Data) for the Best Hyperparameter:\")\n",
    "print(train_classification_df)\n",
    "\n",
    "print(\"\\nClassification Report by Category (Test Data) for the Best Hyperparameter:\")\n",
    "print(test_classification_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b764ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameter: {'fit_intercept': False, 'penalty': 'l2'}\n",
      "             Metric                                 lr Results\n",
      "0   Best Parameters  {'fit_intercept': False, 'penalty': 'l2'}\n",
      "1   Train Precision                                   0.804844\n",
      "2      Train Recall                                    0.80663\n",
      "3    Train F1-Score                                   0.802106\n",
      "4    Train Accuracy                                    0.80663\n",
      "5    Test Precision                                   0.724608\n",
      "6       Test Recall                                   0.730887\n",
      "7     Test F1-Score                                   0.723323\n",
      "8     Test Accuracy                                   0.730887\n",
      "9  Grid Search Time                                 261.468177\n",
      "Classification Report by Category (Train Data) for the Best Hyperparameter:\n",
      "                Train Precision  Train Recall  Train F1-Score  Train Support\n",
      "ARTS & CULTURE         0.746430      0.549888        0.633260     3137.00000\n",
      "BUSINESS               0.763359      0.605049        0.675047     4793.00000\n",
      "CRIME                  0.810183      0.697895        0.749859     2850.00000\n",
      "EDUCATION              0.724522      0.527231        0.610329     1726.00000\n",
      "ENTERTAINMENT          0.797347      0.839657        0.817955    13889.00000\n",
      "ENVIRONMENT            0.745533      0.641254        0.689473     3253.00000\n",
      "HEALTHY LIVING         0.757787      0.865202        0.807940    19711.00000\n",
      "HOME & LIVING          0.849092      0.784722        0.815639     3456.00000\n",
      "PARENTING              0.791873      0.770128        0.780849    10197.00000\n",
      "POLITICS               0.843790      0.916997        0.878872    28481.00000\n",
      "RELIGION               0.827703      0.594372        0.691895     2061.00000\n",
      "SCIENCE                0.812553      0.542776        0.650815     1765.00000\n",
      "SPORTS                 0.846828      0.785521        0.815023     4061.00000\n",
      "STYLE & BEAUTY         0.856402      0.855604        0.856003     9654.00000\n",
      "TRAVEL                 0.836879      0.854419        0.845558     7920.00000\n",
      "WOMEN                  0.671010      0.432470        0.525957     2858.00000\n",
      "accuracy               0.806630      0.806630        0.806630        0.80663\n",
      "macro avg              0.792581      0.703949        0.740280   119812.00000\n",
      "weighted avg           0.804844      0.806630        0.802106   119812.00000\n",
      "\n",
      "Classification Report by Category (Test Data) for the Best Hyperparameter:\n",
      "                Test Precision  Test Recall  Test F1-Score  Test Support\n",
      "ARTS & CULTURE        0.614973     0.439490       0.512630    785.000000\n",
      "BUSINESS              0.636964     0.483306       0.549597   1198.000000\n",
      "CRIME                 0.682724     0.577247       0.625571    712.000000\n",
      "EDUCATION             0.552288     0.391204       0.457995    432.000000\n",
      "ENTERTAINMENT         0.714628     0.772243       0.742319   3473.000000\n",
      "ENVIRONMENT           0.616923     0.493235       0.548189    813.000000\n",
      "HEALTHY LIVING        0.691267     0.798295       0.740936   4928.000000\n",
      "HOME & LIVING         0.759331     0.682870       0.719074    864.000000\n",
      "PARENTING             0.714171     0.697921       0.705952   2549.000000\n",
      "POLITICS              0.795182     0.885534       0.837929   7120.000000\n",
      "RELIGION              0.720000     0.454369       0.557143    515.000000\n",
      "SCIENCE               0.644928     0.403628       0.496513    441.000000\n",
      "SPORTS                0.753247     0.627953       0.684917   1016.000000\n",
      "STYLE & BEAUTY        0.783594     0.787490       0.785537   2414.000000\n",
      "TRAVEL                0.769192     0.769192       0.769192   1980.000000\n",
      "WOMEN                 0.519722     0.313725       0.391266    714.000000\n",
      "accuracy              0.730887     0.730887       0.730887      0.730887\n",
      "macro avg             0.685571     0.598606       0.632798  29954.000000\n",
      "weighted avg          0.724608     0.730887       0.723323  29954.000000\n"
     ]
    }
   ],
   "source": [
    "#Logistic_Regression\n",
    "\n",
    "# Define a grid of hyperparameters for Logistic Regression\n",
    "param_grid = {'penalty': ['l1', 'l2', 'none'],\n",
    "              'fit_intercept': [True, False]}\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# GridSearchCV cross-validation and scoring\n",
    "grid_search_lr = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', refit=True)\n",
    "\n",
    "lrstart = time.time()\n",
    "grid_search_lr.fit(X_train_tfidf, y_train_encoded)\n",
    "lrstop = time.time()\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "lr_best_clf = grid_search_lr.best_estimator_\n",
    "\n",
    "# Predict categories for the training set\n",
    "y_train_pred = lr_best_clf.predict(X_train_tfidf)\n",
    "\n",
    "# Predict categories for the test set\n",
    "y_test_pred = lr_best_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Train set metrics results\n",
    "lr_precision_train = precision_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "lr_recall_train = recall_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "lr_f1_train = f1_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "lr_train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
    "\n",
    "# Test set metrics results\n",
    "lr_precision_test = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "lr_recall_test = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "lr_f1_test = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "lr_test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "\n",
    "# Print metrics by category \n",
    "lr_time = lrstop - lrstart\n",
    "lr_best_para = grid_search_lr.best_params_\n",
    "\n",
    "\n",
    "# Results\n",
    "results_data = {\n",
    "    'Metric': ['Best Parameters', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Accuracy',\n",
    "               'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Accuracy', 'Grid Search Time'],\n",
    "    'lr Results': [lr_best_para, lr_precision_train, lr_recall_train, lr_f1_train, lr_train_accuracy,\n",
    "              lr_precision_test, lr_recall_test, lr_f1_test, lr_test_accuracy, lr_time]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Calculate classification report metrics for train and test data\n",
    "class_names = label_encoder.classes_\n",
    "train_classification_report = classification_report(y_train_encoded, y_train_pred, target_names=class_names, output_dict=True)\n",
    "test_classification_report = classification_report(y_test_encoded, y_test_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "train_classification_df = pd.DataFrame(train_classification_report).transpose()\n",
    "test_classification_df = pd.DataFrame(test_classification_report).transpose()\n",
    "\n",
    "train_classification_df.rename(columns={'precision': 'Train Precision', 'recall': 'Train Recall', 'f1-score': 'Train F1-Score', 'support': 'Train Support'}, inplace=True)\n",
    "test_classification_df.rename(columns={'precision': 'Test Precision', 'recall': 'Test Recall', 'f1-score': 'Test F1-Score', 'support': 'Test Support'}, inplace=True)\n",
    "\n",
    "# Print Results\n",
    "\n",
    "print(\"\\nBest parameter:\", lr_best_para)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(\"Classification Report by Category (Train Data) for the Best Hyperparameter:\")\n",
    "print(train_classification_df)\n",
    "\n",
    "print(\"\\nClassification Report by Category (Test Data) for the Best Hyperparameter:\")\n",
    "print(test_classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0c1d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameter: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 3}\n",
      "             Metric                                         dt Results\n",
      "0   Best Parameters  {'criterion': 'gini', 'max_depth': None, 'min_...\n",
      "1   Train Precision                                           0.976957\n",
      "2      Train Recall                                           0.976288\n",
      "3    Train F1-Score                                           0.976299\n",
      "4    Train Accuracy                                           0.976288\n",
      "5    Test Precision                                           0.600816\n",
      "6       Test Recall                                           0.606396\n",
      "7     Test F1-Score                                           0.601058\n",
      "8     Test Accuracy                                           0.606396\n",
      "9  Grid Search Time                                        4651.786371\n",
      "Classification Report by Category (Train Data) for the Best Hyperparameter:\n",
      "                Train Precision  Train Recall  Train F1-Score  Train Support\n",
      "ARTS & CULTURE         0.907692      0.996812        0.950167    3137.000000\n",
      "BUSINESS               0.923703      0.995201        0.958120    4793.000000\n",
      "CRIME                  0.938477      0.990175        0.963633    2850.000000\n",
      "EDUCATION              0.918177      0.968714        0.942769    1726.000000\n",
      "ENTERTAINMENT          0.972879      0.991792        0.982245   13889.000000\n",
      "ENVIRONMENT            0.942371      0.970181        0.956074    3253.000000\n",
      "HEALTHY LIVING         0.977357      0.989803        0.983540   19711.000000\n",
      "HOME & LIVING          0.965557      0.973380        0.969452    3456.000000\n",
      "PARENTING              0.977525      0.968226        0.972853   10197.000000\n",
      "POLITICS               0.992719      0.986131        0.989414   28481.000000\n",
      "RELIGION               0.973765      0.918486        0.945318    2061.000000\n",
      "SCIENCE                0.975566      0.927479        0.950915    1765.000000\n",
      "SPORTS                 0.985355      0.944349        0.964416    4061.000000\n",
      "STYLE & BEAUTY         0.994670      0.966439        0.980351    9654.000000\n",
      "TRAVEL                 0.998552      0.957955        0.977832    7920.000000\n",
      "WOMEN                  0.998818      0.886634        0.939388    2858.000000\n",
      "accuracy               0.976288      0.976288        0.976288       0.976288\n",
      "macro avg              0.965199      0.964485        0.964156  119812.000000\n",
      "weighted avg           0.976957      0.976288        0.976299  119812.000000\n",
      "\n",
      "Classification Report by Category (Test Data) for the Best Hyperparameter:\n",
      "                Test Precision  Test Recall  Test F1-Score  Test Support\n",
      "ARTS & CULTURE        0.406107     0.338854       0.369444    785.000000\n",
      "BUSINESS              0.438863     0.386477       0.411008   1198.000000\n",
      "CRIME                 0.486137     0.369382       0.419792    712.000000\n",
      "EDUCATION             0.368020     0.335648       0.351090    432.000000\n",
      "ENTERTAINMENT         0.586120     0.600633       0.593288   3473.000000\n",
      "ENVIRONMENT           0.392273     0.324723       0.355316    813.000000\n",
      "HEALTHY LIVING        0.570089     0.703125       0.629657   4928.000000\n",
      "HOME & LIVING         0.591327     0.520833       0.553846    864.000000\n",
      "PARENTING             0.601688     0.615143       0.608341   2549.000000\n",
      "POLITICS              0.749182     0.771910       0.760376   7120.000000\n",
      "RELIGION              0.528610     0.376699       0.439909    515.000000\n",
      "SCIENCE               0.452555     0.281179       0.346853    441.000000\n",
      "SPORTS                0.582921     0.463583       0.516447   1016.000000\n",
      "STYLE & BEAUTY        0.669987     0.640845       0.655092   2414.000000\n",
      "TRAVEL                0.591304     0.583838       0.587548   1980.000000\n",
      "WOMEN                 0.353345     0.288515       0.317656    714.000000\n",
      "accuracy              0.606396     0.606396       0.606396      0.606396\n",
      "macro avg             0.523033     0.475087       0.494729  29954.000000\n",
      "weighted avg          0.600816     0.606396       0.601058  29954.000000\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "# Define a grid of hyperparameters for DecisionTreeClassifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 1, 2, 3, 4],\n",
    "    'min_samples_split': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# GridSearchCV cross-validation and scoring\n",
    "grid_search_dt = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy', refit=True)\n",
    "\n",
    "dtstart = time.time()\n",
    "grid_search_dt.fit(X_train_tfidf, y_train_encoded)\n",
    "dtstop = time.time()\n",
    "\n",
    "# Get the best model \n",
    "dt_best_clf = grid_search_dt.best_estimator_\n",
    "\n",
    "# Predict categories for the training set\n",
    "y_train_pred = dt_best_clf.predict(X_train_tfidf)\n",
    "\n",
    "# Predict categories for the test set\n",
    "y_test_pred = dt_best_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Train set metrics results\n",
    "dt_train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
    "dt_precision_train = precision_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "dt_recall_train = recall_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "dt_f1_train = f1_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set metrics results\n",
    "dt_test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "dt_precision_test = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "dt_recall_test = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "dt_f1_test = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "\n",
    "# Print metrics by category within y_test\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "dt_time = dtstop - dtstart\n",
    "dt_best_para = grid_search_dt.best_params_\n",
    "\n",
    "\n",
    "# Results\n",
    "results_data = {\n",
    "    'Metric': ['Best Parameters', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Accuracy',\n",
    "               'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Accuracy', 'Grid Search Time'],\n",
    "    'dt Results': [dt_best_para, dt_precision_train, dt_recall_train, dt_f1_train, dt_train_accuracy,\n",
    "              dt_precision_test, dt_recall_test, dt_f1_test, dt_test_accuracy, dt_time]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "\n",
    "# Calculate classification report metrics for train and test data\n",
    "train_classification_report = classification_report(y_train_encoded, y_train_pred, target_names=class_names, output_dict=True)\n",
    "test_classification_report = classification_report(y_test_encoded, y_test_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "train_classification_df = pd.DataFrame(train_classification_report).transpose()\n",
    "test_classification_df = pd.DataFrame(test_classification_report).transpose()\n",
    "\n",
    "train_classification_df.rename(columns={'precision': 'Train Precision', 'recall': 'Train Recall', 'f1-score': 'Train F1-Score', 'support': 'Train Support'}, inplace=True)\n",
    "test_classification_df.rename(columns={'precision': 'Test Precision', 'recall': 'Test Recall', 'f1-score': 'Test F1-Score', 'support': 'Test Support'}, inplace=True)\n",
    "\n",
    "# Print Results\n",
    "\n",
    "print(\"\\nBest parameter:\", dt_best_para)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(\"Classification Report by Category (Train Data) for the Best Hyperparameter:\")\n",
    "print(train_classification_df)\n",
    "\n",
    "print(\"\\nClassification Report by Category (Test Data) for the Best Hyperparameter:\")\n",
    "print(test_classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb79b7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Metric En Results\n",
      "0   Best Parameters        N/A\n",
      "1   Train Precision   0.821128\n",
      "2      Train Recall   0.811605\n",
      "3    Train F1-Score    0.80523\n",
      "4    Train Accuracy   0.811605\n",
      "5    Test Precision   0.720697\n",
      "6       Test Recall    0.71099\n",
      "7     Test F1-Score   0.698021\n",
      "8     Test Accuracy    0.71099\n",
      "9  Grid Search Time   7.119377\n",
      "Classification Report by Category (Train Data) for the Best Hyperparameter:\n",
      "                Train Precision  Train Recall  Train F1-Score  Train Support\n",
      "ARTS & CULTURE         0.780022      0.689512        0.731980    3137.000000\n",
      "BUSINESS               0.769698      0.686835        0.725910    4793.000000\n",
      "CRIME                  0.805906      0.785263        0.795451    2850.000000\n",
      "EDUCATION              0.753846      0.567787        0.647720    1726.000000\n",
      "ENTERTAINMENT          0.771048      0.895457        0.828609   13889.000000\n",
      "ENVIRONMENT            0.771410      0.719951        0.744792    3253.000000\n",
      "HEALTHY LIVING         0.722597      0.901020        0.802005   19711.000000\n",
      "HOME & LIVING          0.893175      0.783854        0.834951    3456.000000\n",
      "PARENTING              0.815527      0.728351        0.769478   10197.000000\n",
      "POLITICS               0.838552      0.918753        0.876822   28481.000000\n",
      "RELIGION               0.932275      0.427462        0.586161    2061.000000\n",
      "SCIENCE                0.942414      0.482153        0.637931    1765.000000\n",
      "SPORTS                 0.914259      0.732578        0.813397    4061.000000\n",
      "STYLE & BEAUTY         0.911109      0.820696        0.863542    9654.000000\n",
      "TRAVEL                 0.909844      0.804040        0.853677    7920.000000\n",
      "WOMEN                  0.909887      0.254374        0.397594    2858.000000\n",
      "accuracy               0.811605      0.811605        0.811605       0.811605\n",
      "macro avg              0.840098      0.699880        0.744376  119812.000000\n",
      "weighted avg           0.821128      0.811605        0.805230  119812.000000\n",
      "\n",
      "Classification Report by Category (Test Data) for the Best Hyperparameter:\n",
      "                Test Precision  Test Recall  Test F1-Score  Test Support\n",
      "ARTS & CULTURE        0.591772     0.476433       0.527876     785.00000\n",
      "BUSINESS              0.605882     0.515860       0.557259    1198.00000\n",
      "CRIME                 0.654624     0.636236       0.645299     712.00000\n",
      "EDUCATION             0.547297     0.375000       0.445055     432.00000\n",
      "ENTERTAINMENT         0.649234     0.829254       0.728284    3473.00000\n",
      "ENVIRONMENT           0.595960     0.507995       0.548473     813.00000\n",
      "HEALTHY LIVING        0.634985     0.832386       0.720407    4928.00000\n",
      "HOME & LIVING         0.779006     0.652778       0.710327     864.00000\n",
      "PARENTING             0.702703     0.612005       0.654225    2549.00000\n",
      "POLITICS              0.773490     0.881039       0.823769    7120.00000\n",
      "RELIGION              0.816901     0.225243       0.353120     515.00000\n",
      "SCIENCE               0.846626     0.312925       0.456954     441.00000\n",
      "SPORTS                0.824089     0.511811       0.631451    1016.00000\n",
      "STYLE & BEAUTY        0.841872     0.707954       0.769127    2414.00000\n",
      "TRAVEL                0.835427     0.671717       0.744681    1980.00000\n",
      "WOMEN                 0.752212     0.119048       0.205562     714.00000\n",
      "accuracy              0.710990     0.710990       0.710990       0.71099\n",
      "macro avg             0.715755     0.554230       0.595117   29954.00000\n",
      "weighted avg          0.720697     0.710990       0.698021   29954.00000\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Classifiers\n",
    "lr_clf = LogisticRegression(fit_intercept=False, penalty='l2')\n",
    "nb_clf = MultinomialNB(alpha=0.1)  \n",
    "\n",
    "# Create the ensemble using VotingClassifier\n",
    "ensemble_clf = VotingClassifier(estimators=[\n",
    "    ('multinomial_nb', nb_clf),\n",
    "    ('logistic_lr', lr_clf),\n",
    "], voting='hard')  \n",
    "\n",
    "enstart = time.time()\n",
    "ensemble_clf.fit(X_train_tfidf, y_train_encoded)\n",
    "enstop = time.time()\n",
    "\n",
    "# Predict categories for the training set\n",
    "y_train_pred = ensemble_clf.predict(X_train_tfidf)\n",
    "\n",
    "# Predict categories for the test set\n",
    "y_test_pred = ensemble_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Train set metrics results\n",
    "en_precision_train = precision_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "en_recall_train = recall_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "en_f1_train = f1_score(y_train_encoded, y_train_pred, average='weighted')\n",
    "en_train_accuracy = accuracy_score(y_train_encoded, y_train_pred)\n",
    "\n",
    "# Test set metrics results\n",
    "en_precision_test = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "en_recall_test = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "en_f1_test = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "en_test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "\n",
    "# Print metrics by category within y_test\n",
    "class_names = label_encoder.classes_\n",
    "en_time = enstop - enstart\n",
    "en_best_para = 'N/A'\n",
    "\n",
    "\n",
    "# Results\n",
    "results_data = {\n",
    "    'Metric': ['Best Parameters', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Accuracy',\n",
    "               'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Accuracy', 'Grid Search Time'],\n",
    "    'En Results': [en_best_para, en_precision_train, en_recall_train, en_f1_train, en_train_accuracy,\n",
    "              en_precision_test, en_recall_test, en_f1_test, en_test_accuracy, en_time]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "\n",
    "# Calculate classification report metrics for train and test data\n",
    "train_classification_report = classification_report(y_train_encoded, y_train_pred, target_names=class_names, output_dict=True)\n",
    "test_classification_report = classification_report(y_test_encoded, y_test_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "train_classification_df = pd.DataFrame(train_classification_report).transpose()\n",
    "test_classification_df = pd.DataFrame(test_classification_report).transpose()\n",
    "\n",
    "train_classification_df.rename(columns={'precision': 'Train Precision', 'recall': 'Train Recall', 'f1-score': 'Train F1-Score', 'support': 'Train Support'}, inplace=True)\n",
    "test_classification_df.rename(columns={'precision': 'Test Precision', 'recall': 'Test Recall', 'f1-score': 'Test F1-Score', 'support': 'Test Support'}, inplace=True)\n",
    "\n",
    "# Print Results\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "print(\"Classification Report by Category (Train Data) for the Best Hyperparameter:\")\n",
    "print(train_classification_df)\n",
    "\n",
    "print(\"\\nClassification Report by Category (Test Data) for the Best Hyperparameter:\")\n",
    "print(test_classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebee7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Metric  NB Results                                 lr Results  \\\n",
      "0        Best Alpha    0.100000  {'fit_intercept': False, 'penalty': 'l2'}   \n",
      "1   Train Precision    0.826818                                   0.804844   \n",
      "2      Train Recall    0.812990                                    0.80663   \n",
      "3    Train F1-Score    0.805137                                   0.802106   \n",
      "4    Train Accuracy    0.812990                                    0.80663   \n",
      "5    Test Precision    0.697963                                   0.724608   \n",
      "6       Test Recall    0.690125                                   0.730887   \n",
      "7     Test F1-Score    0.667307                                   0.723323   \n",
      "8     Test Accuracy    0.690125                                   0.730887   \n",
      "9  Grid Search Time    2.162459                                 261.468177   \n",
      "\n",
      "                                          dt Results En Results  \n",
      "0  {'criterion': 'gini', 'max_depth': None, 'min_...        N/A  \n",
      "1                                           0.976957   0.821128  \n",
      "2                                           0.976288   0.811605  \n",
      "3                                           0.976299    0.80523  \n",
      "4                                           0.976288   0.811605  \n",
      "5                                           0.600816   0.720697  \n",
      "6                                           0.606396    0.71099  \n",
      "7                                           0.601058   0.698021  \n",
      "8                                           0.606396    0.71099  \n",
      "9                                        4651.786371   7.119377  \n"
     ]
    }
   ],
   "source": [
    "results_data_all= {\n",
    "    'Metric': ['Best Alpha', 'Train Precision', 'Train Recall', 'Train F1-Score', 'Train Accuracy',\n",
    "               'Test Precision', 'Test Recall', 'Test F1-Score', 'Test Accuracy', 'Grid Search Time'],\n",
    "    'NB Results': [nb_best_alpha, nb_precision_train, nb_recall_train, nb_f1_train, nb_train_accuracy,\n",
    "              nb_precision_test, nb_recall_test, nb_f1_test, nb_test_accuracy, nb_time],\n",
    "    'lr Results': [lr_best_para, lr_precision_train, lr_recall_train, lr_f1_train, lr_train_accuracy,\n",
    "              lr_precision_test, lr_recall_test, lr_f1_test, lr_test_accuracy, lr_time],\n",
    "    'dt Results': [dt_best_para, dt_precision_train, dt_recall_train, dt_f1_train, dt_train_accuracy,\n",
    "              dt_precision_test, dt_recall_test, dt_f1_test, dt_test_accuracy, dt_time],\n",
    "     'En Results': [en_best_para, en_precision_train, en_recall_train, en_f1_train, en_train_accuracy,\n",
    "              en_precision_test, en_recall_test, en_f1_test, en_test_accuracy, en_time]\n",
    "}\n",
    "\n",
    "results_df_all = pd.DataFrame(results_data_all)\n",
    "print(results_df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ee273",
   "metadata": {},
   "source": [
    "Conclusion: When considering the headline as the target variable (y), logistic regression emerges as the optimal model, achieving the highest level of accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
